{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb338f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7abaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols=['tweetid', 'entity', 'target', 'content']\n",
    "\n",
    "df_train  = pd.read_csv(\"twitter_training.csv\",names=cols)\n",
    "df_test  = pd.read_csv(\"twitter_validation.csv\",names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642e8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns = ['Tweet ID', 'Entity', 'Sentiment', 'Tweet Content']\n",
    "df_test.columns = ['Tweet ID', 'Entity', 'Sentiment', 'Tweet Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ed1eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet ID       Entity Sentiment  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "\n",
       "                                       Tweet Content  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b67288",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229aa6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=['Tweet ID'], inplace=True)\n",
    "df_test.drop(columns=['Tweet ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4740d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entity           0\n",
       "Sentiment        0\n",
       "Tweet Content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0786a897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.duplicated().sum()\n",
    "df_test.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983e2bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4138"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_sentiments = pd.concat([df_train,df_test],ignore_index=True)\n",
    "twitter_sentiments.shape\n",
    "twitter_sentiments.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23b562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in the twitter_sentiments: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71544, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_sentiments.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"Number of duplicates in the twitter_sentiments:\", twitter_sentiments.duplicated().sum())\n",
    "twitter_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0cf55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with an empty string\n",
    "twitter_sentiments['Tweet Content'] = twitter_sentiments['Tweet Content'].fillna('')\n",
    "\n",
    "# Ensure all values are strings\n",
    "twitter_sentiments['Tweet Content'] = twitter_sentiments['Tweet Content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "448a6dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet Content</th>\n",
       "      <th>Tweet Content Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entity Sentiment                                      Tweet Content  \\\n",
       "0  Borderlands  Positive  im getting on borderlands and i will murder yo...   \n",
       "1  Borderlands  Positive  I am coming to the borders and I will kill you...   \n",
       "2  Borderlands  Positive  im getting on borderlands and i will kill you ...   \n",
       "3  Borderlands  Positive  im coming on borderlands and i will murder you...   \n",
       "4  Borderlands  Positive  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "   Tweet Content Length  \n",
       "0                    53  \n",
       "1                    51  \n",
       "2                    50  \n",
       "3                    51  \n",
       "4                    57  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_sentiments['Tweet Content Length'] = twitter_sentiments['Tweet Content'].apply(len)\n",
    "twitter_sentiments.head()\n",
    "twitter_sentiments.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d6dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d28e6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Tweet Content'] = df_train['Tweet Content'].fillna('')\n",
    "df_test['Tweet Content'] = df_test['Tweet Content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b4e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ramsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ramsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ramsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9edae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text: str) -> str:\n",
    "    return re.sub(r'https*\\S+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e63690d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions_and_hashtags(text: str) -> str:\n",
    "    return re.sub(r'[@#]\\S+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bbaf070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text: str) -> str:\n",
    "    return re.sub('<.*?>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfd3ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text: str) -> str:\n",
    "    contractions = {\"don't\": 'do not', \"can't\": 'cannot'}\n",
    "    for contraction, expanded in contractions.items():\n",
    "        text = text.replace(contraction, expanded)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "419750d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub('[%s]' % re.escape(string.punctuation), '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc6f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub('[%s]' % re.escape(string.punctuation), '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3088ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespaces(text: str) -> str:\n",
    "    return re.sub('\\s{2,}', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf003543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters_and_emojis(text: str) -> str:\n",
    "    special_char_pattern = r'[^a-zA-Z0-9\\s]|[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]'\n",
    "\n",
    "    return re.sub(special_char_pattern, '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5434126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(text: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords_set]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72af72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = remove_special_characters_and_emojis(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_mentions_and_hashtags(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_extra_whitespaces(text)\n",
    "    text = tokenize_and_lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "430130a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_tweet'] = df_train['Tweet Content'].apply(clean_text)\n",
    "df_test['cleaned_tweet'] = df_test['Tweet Content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acb9d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment value count: \n",
      " Neutral     31308\n",
      "Negative    22542\n",
      "Positive    20832\n",
      "Name: Sentiment, dtype: int64\n",
      "Sentiment value count: \n",
      " Neutral     457\n",
      "Positive    277\n",
      "Negative    266\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train['Sentiment'] = df_train['Sentiment'].apply(lambda x: 'Neutral' if x.lower() in ['neutral', 'irrelevant'] else x)\n",
    "print(f\"Sentiment value count: \\n {df_train['Sentiment'].value_counts()}\")\n",
    "df_test['Sentiment'] = df_test['Sentiment'].apply(lambda x: 'Neutral' if x.lower() in ['neutral', 'irrelevant'] else x)\n",
    "print(f\"Sentiment value count: \\n {df_test['Sentiment'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0a5e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df_train['encoded_sentiment'] = label_encoder.fit_transform(df_train['Sentiment'])\n",
    "df_test['encoded_sentiment'] = label_encoder.transform(df_test['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "064065e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_sentiments = pd.concat([df_train['cleaned_tweet'], df_test['cleaned_tweet']])\n",
    "\n",
    "tokenized_tweets = [tweet.split() for tweet in twitter_sentiments]\n",
    "\n",
    "w2v_model = Word2Vec(tokenized_tweets, vector_size=100, window=5, min_count=1, sample=1e-3, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8ad8a",
   "metadata": {},
   "source": [
    "##  Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a177601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_sentiments = pd.concat([df_train['cleaned_tweet'], df_test['cleaned_tweet']])\n",
    "\n",
    "tokenized_tweets = [tweet.split() for tweet in twitter_sentiments]\n",
    "\n",
    "w2v_model = Word2Vec(tokenized_tweets, vector_size=100, window=5, min_count=1, sample=1e-3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ac414f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(twitter_sentiments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(df_train['cleaned_tweet'])\n",
    "test_sequences = tokenizer.texts_to_sequences(df_test['cleaned_tweet'])\n",
    "\n",
    "X_train = pad_sequences(train_sequences, maxlen=100)\n",
    "X_test = pad_sequences(test_sequences, maxlen=100)\n",
    "\n",
    "y_train = df_train['encoded_sentiment']\n",
    "y_test = df_test['encoded_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16653531",
   "metadata": {},
   "source": [
    "## Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c901c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(3, activation='softmax')) # 3 units for 'Positive', 'Negative', 'Neutral'\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fc35e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 500ms/step - accuracy: 0.5945 - loss: 0.8449 - val_accuracy: 0.5293 - val_loss: 1.2893\n",
      "Epoch 2/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 585ms/step - accuracy: 0.8794 - loss: 0.3006 - val_accuracy: 0.5253 - val_loss: 1.6422\n",
      "Epoch 3/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 430ms/step - accuracy: 0.9281 - loss: 0.1796 - val_accuracy: 0.5152 - val_loss: 2.0086\n",
      "Epoch 4/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 500ms/step - accuracy: 0.9424 - loss: 0.1387 - val_accuracy: 0.5113 - val_loss: 2.2477\n",
      "Epoch 5/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 499ms/step - accuracy: 0.9508 - loss: 0.1159 - val_accuracy: 0.5034 - val_loss: 2.4391\n",
      "Epoch 6/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 554ms/step - accuracy: 0.9558 - loss: 0.1050 - val_accuracy: 0.4929 - val_loss: 2.7093\n",
      "Epoch 7/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 378ms/step - accuracy: 0.9586 - loss: 0.0951 - val_accuracy: 0.5045 - val_loss: 2.7617\n",
      "Epoch 8/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 421ms/step - accuracy: 0.9632 - loss: 0.0870 - val_accuracy: 0.5016 - val_loss: 3.0037\n",
      "Epoch 9/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 438ms/step - accuracy: 0.9644 - loss: 0.0825 - val_accuracy: 0.5071 - val_loss: 3.1281\n",
      "Epoch 10/10\n",
      "\u001b[1m467/467\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 493ms/step - accuracy: 0.9647 - loss: 0.0804 - val_accuracy: 0.4998 - val_loss: 3.1437\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89ea9d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.8644 - loss: 0.8456\n",
      "Test Loss: 0.9332301020622253\n",
      "Test Accuracy: 0.859000027179718\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5168d8",
   "metadata": {},
   "source": [
    "## Confusion Matrix for LSTM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f5b7d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m----> 2\u001b[0m y_pred_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m y_true_classes \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m      6\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true_classes, y_pred_classes)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = y_test\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2'], \n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
